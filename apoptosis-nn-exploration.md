# âœ… **PROJECT SUMMARY: Apoptosis-Driven Neural Architectures**

Your project is an experimental neural network training framework exploring **biologically inspired lifecycle mechanics**, including:

* **Neuron-level apoptosis** (kill underperforming neurons and regrow fresh ones)
* **Senescence** (neurons age, slow learning, and eventually get replaced)
* **Fitness-based survival** (using gradients, activation variance, stagnation penalty)
* **Experimental hyperparameter sweeps and orthogonal-array tuning**
* **Instrumentation and logging to JSON + SQLite**
* **Future: adaptive experimentation controller + TUI dashboard**
* **Far-future: recursive architectures (TRM-like), population evolution**

The goal is *not* to optimize a model for accuracy, but to **explore dynamics**, measure emergent behavior, and understand how neuron-level birth/death affects learning.

Youâ€™ve already built:

* A full apoptosis manager
* Several generations of fitness metrics
* A senescence daemon
* Histograms, percentiles, and per-layer metrics
* Orthogonal array test harness
* JSON logging + early SQLite prototype
* NaN/inf sanitization and fitness stability improvements

You want to consolidate all of this into a clean project scaffold.

---

# ğŸ§  **TIMELINE OF KEY DECISIONS & IDEAS**

## **Phase 1 â€” Early exploration**

* Initial idea: prune lowest-fitness neurons periodically.
* Early fitness: simple gradient magnitude.
* Issues: slow apoptosis, MPS CPU fallback, unstable gradients.

## **Phase 2 â€” Performance Engineering**

* Huge speedup from moving apoptosis to CPU for linear algebra.
* Replaced dense SVD with faster heuristics.
* Pruned code paths, introduced fast regrowth.
* Added orthogonal array sweeps.
* Introduced TensorBoard instrumentation.

## **Phase 3 â€” Lifecycle Design**

* Introduced â€œneuron age.â€
* Decided to model:

  * newborn â†’ mature â†’ senior â†’ dying
* Proposed temperature-like learning rate adjustment.

## **Phase 4 â€” Senescence Mechanism**

* Designed â€œsenescence daemonâ€:

  * track rolling slope of neuron fitness
  * declare senescence if flatlined X steps
  * escalate â†’ kill â†’ retry
* Many refinements to avoid over-triggering.

## **Phase 5 â€” Fitness Redesign**

We moved away from single-term fitness to:

```
fitness = Î± * grad_norm
        + Î² * activation_variance
        - Î³ * stagnation_penalty
```

Where stagnation is based on similarity to an EMA of activation means.

## **Phase 6 â€” Instrumentation**

Metrics recorded:

* per-layer histograms (quantized)
* percentiles
* mean/std/variance
* apoptosis & senescence events
* high-resolution run logs

JSON output now has:

* params
* metrics (per layer & per step)
* events (with neuron index + step)
* timing breakdown

## **Phase 7 â€” Error Hardening**

We discovered:

* MPS sometimes produces NaNs post-apoptosis
* EMA contamination creates long-lived NaNs
* histograms fail when min/max = NaN
* fitness normalization can zero-divide

We added:

* `nan_to_num` everywhere
* histogram guards
* EMA reset on neuron reset
* safe normalizations everywhere

## **Phase 8 â€” Data Storage**

You requested:

* SQLite backend
* storing quantized states
* storing apoptosis/senescence events
* storing params/metrics per run
* groundwork for adaptive experimentation controller

## **Phase 9 â€” Forward Vision**

Planned features:

* Textual/urwid/PromptToolkit TUI visualizer
* Adaptive controller (bandit / Bayesian optimization)
* Automated sweeps + self-guiding exploration loop
* Population genetics (model populations, mutation, selection)
* Integration with Recursive Transformer experiments

---

# ğŸ“ **CURRENT SYSTEM ARCHITECTURE (High-Level)**

```
apoptosis-v2/
â”‚
â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ training loop
â”‚   â”œâ”€â”€ loss/backprop
â”‚   â”œâ”€â”€ logging / instrumentation
â”‚   â””â”€â”€ invokes NeuronApoptosisManager
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ neuron_apoptosis_manager.py
â”‚   â”‚   â”œâ”€â”€ fitness computation
â”‚   â”‚   â”œâ”€â”€ senescence daemon
â”‚   â”‚   â”œâ”€â”€ apoptosis logic
â”‚   â”‚   â”œâ”€â”€ regrowth mechanisms
â”‚   â”‚   â”œâ”€â”€ per-layer state tracking
â”‚   â”‚   â””â”€â”€ histogram/percentile extraction
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ histogram.py
â”‚   â”‚   â”œâ”€â”€ normalization.py
â”‚   â”‚   â”œâ”€â”€ event_logging.py
â”‚   â”‚   â””â”€â”€ nan_sanitization.py
â”‚   â”‚
â”‚   â””â”€â”€ db/
â”‚       â”œâ”€â”€ schema.sql
â”‚       â”œâ”€â”€ insert_run.py
â”‚       â”œâ”€â”€ insert_metrics.py
â”‚       â””â”€â”€ insert_events.py
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ sweeps/
â”‚   â”‚   â””â”€â”€ sweep.py (orthogonal arrays + grid + random)
â”‚   â”œâ”€â”€ adaptive/
â”‚   â”‚   â”œâ”€â”€ bandit_controller.py
â”‚   â”‚   â”œâ”€â”€ bo_controller.py
â”‚   â”‚   â””â”€â”€ evolution_controller.py
â”‚   â””â”€â”€ configs/
â”‚       â””â”€â”€ (all your hyperparam sets)
â”‚
â””â”€â”€ dashboards/
    â”œâ”€â”€ textual_ui.py
    â””â”€â”€ plot_notebook.ipynb
```

---

# ğŸ¯ **FUTURE GOALS & PLANNED DIRECTIONS**

This is what we discussed and agreed on, in priority order.

---

## **1. TUI Visualizer**

Something like **Textual**, showing:

* per-layer histograms updating in real time
* apoptosis/senescence indicators
* live fitness distribution
* neuron age distribution
* loss curve
* timeline of events

Basically a â€œneural health monitor.â€

**Difficulty:** Medium
**Effort:** 1â€“2 days
**Value:** Huge for intuition

---

## **2. Auto-Scaling Hyperparameters (Controller)**

System watches runs and learns:

* what hyperparams hurt/help
* what fitness distributions are â€œhealthyâ€
* when to increase/decrease mutation
* when to make apoptosis more aggressive
* which layers need more pruning
* etc.

Techniques:

* Multi-armed bandits (UCB, Thompson) for simple control
* Bayesian Optimization for structured sweeps
* Evolutionary Algorithm for population-based training

**Difficulty:** High
**Effort:** 3â€“7 days
**Value:** Massive â€” turns experiment into a *self-driving lab*

---

## **3. Genetic Population (Model Families)**

A population of models:

* each trains independently
* apoptosis/senescence act as â€œwithin-model mutationsâ€
* selection pressure chooses next generation

You get:

* speciation
* convergence
* architecture evolution patterns

This will be *extremely* interesting.

**Difficulty:** Very High
**Effort:** Multi-week
**Value:** ğŸ’¥ Frontier research territory


JSON logging + early SQLite prototype
NaN/inf sanitization and fitness stability improvements
You want to consolidate all of this into a clean project scaffold.
